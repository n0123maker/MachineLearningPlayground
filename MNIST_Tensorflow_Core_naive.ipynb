{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, depth):\n",
    "    depth = tf.constant(depth, tf.int32, name=\"depth\")\n",
    "    one_hot_matrix = tf.one_hot(labels, depth, axis=-1)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        one_hot = sess.run(one_hot_matrix)\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = mnist.load_data()\n",
    "\n",
    "#Flatten the training and test images\n",
    "x_train_flat = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n",
    "x_test_flat = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n",
    "\n",
    "#Normalize image vectors\n",
    "x_train = x_train_flat / 255.0\n",
    "x_test = x_test_flat / 255.0\n",
    "\n",
    "#Convert training and test labels to on hot matrices\n",
    "num_classes = 10\n",
    "y_train = one_hot_matrix(y_train_orig, num_classes)\n",
    "y_test = one_hot_matrix(y_test_orig, num_classes)\n",
    "\n",
    "n_x = x_train.shape[0]\n",
    "n_y = x_test.shape[0]\n",
    "\n",
    "print (\"number of training examples = \" + str(x_train.shape[1]))\n",
    "print (\"number of test examples = \" + str(x_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(x_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(x_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))\n",
    "\n",
    "#print(y_train_orig[0:5])\n",
    "#print(y_train[:, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 60000\n",
      "number of test examples = 10000\n",
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n",
      "X_test shape: (10000, 784)\n",
      "Y_test shape: (10000, 10)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#New way\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = mnist.load_data()\n",
    "\n",
    "#Flatten the training and test images\n",
    "x_train_flat = x_train_orig.reshape(x_train_orig.shape[0], -1)\n",
    "x_test_flat = x_test_orig.reshape(x_test_orig.shape[0], -1)\n",
    "\n",
    "#Normalize image vectors\n",
    "x_train = x_train_flat / 255.0\n",
    "x_test = x_test_flat / 255.0\n",
    "\n",
    "#Convert training and test labels to on hot matrices\n",
    "num_classes = 10\n",
    "y_train = one_hot_matrix(y_train_orig, num_classes)\n",
    "y_test = one_hot_matrix(y_test_orig, num_classes)\n",
    "\n",
    "n_x = x_train.shape[1]\n",
    "n_y = x_test.shape[1]\n",
    "\n",
    "print (\"number of training examples = \" + str(x_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(x_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(x_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(x_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))\n",
    "\n",
    "print(x_train[0:2, :])\n",
    "print(y_train[0:2, :])\n",
    "\n",
    "print(x_train_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name=\"Y\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_x), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y), name=\"Y\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "def initialize_parameters(n_x):\n",
    "    W1 = tf.get_variable(\"W1\", [100, n_x], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [100, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [100, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [100, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [10, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def initialize_parameters(n_x):\n",
    "    W1 = tf.get_variable(\"W1\", [n_x, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [1, 100], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [100, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [1, 100], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [100, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [1, 10], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.matmul(W2, A1) + b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.matmul(W3, A2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(X, W1) + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.matmul(A1, W2) + b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.matmul(A2, W3) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "def compute_cost(Z, Y):\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))            \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def compute_cost(Z, Y):   \n",
    "    logits = Z\n",
    "    labels = Y\n",
    "    \n",
    "    #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)\n",
    "    #cost = tf.reduce_mean(cross_entropy)            \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels)\n",
    "    cost = tf.reduce_mean(cross_entropy)    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01, num_epochs = 50):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    (n_x, m) = X_train.shape\n",
    "    n_y = Y_train.shape[0]\n",
    "    #costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    parameters = initialize_parameters(n_x)\n",
    "    \n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    cost = compute_cost(Z, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            _, epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})   \n",
    "            if (epoch % 10 == 0):\n",
    "                print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "         \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print('Time elapsed {:.3f} (hh:mm:ss.ms)'.format(end_time - begin_time))\n",
    "        #print(\"Model trained in %.2f seconds\" % (end_time - begin_time))\n",
    "        \n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy: {:.4f}\".format(train_accuracy))\n",
    "        print(\"Test Accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01, num_epochs = 50):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    (m, n_x) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    #costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    parameters = initialize_parameters(n_x)\n",
    "    \n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    cost = compute_cost(Z, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        begin_time = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            _, epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})   \n",
    "            if (epoch % 10 == 0):\n",
    "                print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "         \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print('Time elapsed {:.3f} (hh:mm:ss.ms)'.format(end_time - begin_time))\n",
    "        #print(\"Model trained in %.2f seconds\" % (end_time - begin_time))\n",
    "        \n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z, axis=1), tf.argmax(Y, axis=1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy: {:.4f}\".format(train_accuracy))\n",
    "        print(\"Test Accuracy: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.372673273086548\n",
      "Cost after epoch 10: 0.4198455214500427\n",
      "Cost after epoch 20: 0.2645059823989868\n",
      "Cost after epoch 30: 0.2038988173007965\n",
      "Cost after epoch 40: 0.15980367362499237\n",
      "Time elapsed 31.800 (hh:mm:ss.ms)\n",
      "Train Accuracy: 0.9615\n",
      "Test Accuracy: 0.9557\n"
     ]
    }
   ],
   "source": [
    "model(x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
