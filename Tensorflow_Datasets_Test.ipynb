{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_x1 = np.arange(0, 100).reshape((100, 1))\n",
    "train_data_x2 = np.arange(100, 200).reshape((100, 1))\n",
    "train_data_x = np.hstack([train_data_x1, train_data_x2])\n",
    "train_data_y = np.arange(200, 300).reshape((100, 1))\n",
    "\n",
    "print(train_data_x.shape)\n",
    "print(train_data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 100.]\n",
      " [  1. 101.]\n",
      " [  2. 102.]]\n",
      "[[200.]\n",
      " [201.]\n",
      " [202.]]\n",
      "[[  3. 103.]\n",
      " [  4. 104.]\n",
      " [  5. 105.]]\n",
      "[[203.]\n",
      " [204.]\n",
      " [205.]]\n",
      "[[  6. 106.]\n",
      " [  7. 107.]\n",
      " [  8. 108.]]\n",
      "[[206.]\n",
      " [207.]\n",
      " [208.]]\n",
      "[[  9. 109.]\n",
      " [ 10. 110.]\n",
      " [ 11. 111.]]\n",
      "[[209.]\n",
      " [210.]\n",
      " [211.]]\n",
      "[[ 12. 112.]\n",
      " [ 13. 113.]\n",
      " [ 14. 114.]]\n",
      "[[212.]\n",
      " [213.]\n",
      " [214.]]\n",
      "[[ 15. 115.]\n",
      " [ 16. 116.]\n",
      " [ 17. 117.]]\n",
      "[[215.]\n",
      " [216.]\n",
      " [217.]]\n",
      "[[ 18. 118.]\n",
      " [ 19. 119.]\n",
      " [ 20. 120.]]\n",
      "[[218.]\n",
      " [219.]\n",
      " [220.]]\n",
      "[[ 21. 121.]\n",
      " [ 22. 122.]\n",
      " [ 23. 123.]]\n",
      "[[221.]\n",
      " [222.]\n",
      " [223.]]\n",
      "[[ 24. 124.]\n",
      " [ 25. 125.]\n",
      " [ 26. 126.]]\n",
      "[[224.]\n",
      " [225.]\n",
      " [226.]]\n",
      "[[ 27. 127.]\n",
      " [ 28. 128.]\n",
      " [ 29. 129.]]\n",
      "[[227.]\n",
      " [228.]\n",
      " [229.]]\n",
      "[[ 30. 130.]\n",
      " [ 31. 131.]\n",
      " [ 32. 132.]]\n",
      "[[230.]\n",
      " [231.]\n",
      " [232.]]\n",
      "[[ 33. 133.]\n",
      " [ 34. 134.]\n",
      " [ 35. 135.]]\n",
      "[[233.]\n",
      " [234.]\n",
      " [235.]]\n",
      "[[ 36. 136.]\n",
      " [ 37. 137.]\n",
      " [ 38. 138.]]\n",
      "[[236.]\n",
      " [237.]\n",
      " [238.]]\n",
      "[[ 39. 139.]\n",
      " [ 40. 140.]\n",
      " [ 41. 141.]]\n",
      "[[239.]\n",
      " [240.]\n",
      " [241.]]\n",
      "[[ 42. 142.]\n",
      " [ 43. 143.]\n",
      " [ 44. 144.]]\n",
      "[[242.]\n",
      " [243.]\n",
      " [244.]]\n",
      "[[ 45. 145.]\n",
      " [ 46. 146.]\n",
      " [ 47. 147.]]\n",
      "[[245.]\n",
      " [246.]\n",
      " [247.]]\n",
      "[[ 48. 148.]\n",
      " [ 49. 149.]\n",
      " [ 50. 150.]]\n",
      "[[248.]\n",
      " [249.]\n",
      " [250.]]\n",
      "[[ 51. 151.]\n",
      " [ 52. 152.]\n",
      " [ 53. 153.]]\n",
      "[[251.]\n",
      " [252.]\n",
      " [253.]]\n",
      "[[ 54. 154.]\n",
      " [ 55. 155.]\n",
      " [ 56. 156.]]\n",
      "[[254.]\n",
      " [255.]\n",
      " [256.]]\n",
      "[[ 57. 157.]\n",
      " [ 58. 158.]\n",
      " [ 59. 159.]]\n",
      "[[257.]\n",
      " [258.]\n",
      " [259.]]\n",
      "[[ 60. 160.]\n",
      " [ 61. 161.]\n",
      " [ 62. 162.]]\n",
      "[[260.]\n",
      " [261.]\n",
      " [262.]]\n",
      "[[ 63. 163.]\n",
      " [ 64. 164.]\n",
      " [ 65. 165.]]\n",
      "[[263.]\n",
      " [264.]\n",
      " [265.]]\n",
      "[[ 66. 166.]\n",
      " [ 67. 167.]\n",
      " [ 68. 168.]]\n",
      "[[266.]\n",
      " [267.]\n",
      " [268.]]\n",
      "[[ 69. 169.]\n",
      " [ 70. 170.]\n",
      " [ 71. 171.]]\n",
      "[[269.]\n",
      " [270.]\n",
      " [271.]]\n",
      "[[ 72. 172.]\n",
      " [ 73. 173.]\n",
      " [ 74. 174.]]\n",
      "[[272.]\n",
      " [273.]\n",
      " [274.]]\n",
      "[[ 75. 175.]\n",
      " [ 76. 176.]\n",
      " [ 77. 177.]]\n",
      "[[275.]\n",
      " [276.]\n",
      " [277.]]\n",
      "[[ 78. 178.]\n",
      " [ 79. 179.]\n",
      " [ 80. 180.]]\n",
      "[[278.]\n",
      " [279.]\n",
      " [280.]]\n",
      "[[ 81. 181.]\n",
      " [ 82. 182.]\n",
      " [ 83. 183.]]\n",
      "[[281.]\n",
      " [282.]\n",
      " [283.]]\n",
      "[[ 84. 184.]\n",
      " [ 85. 185.]\n",
      " [ 86. 186.]]\n",
      "[[284.]\n",
      " [285.]\n",
      " [286.]]\n",
      "[[ 87. 187.]\n",
      " [ 88. 188.]\n",
      " [ 89. 189.]]\n",
      "[[287.]\n",
      " [288.]\n",
      " [289.]]\n",
      "[[ 90. 190.]\n",
      " [ 91. 191.]\n",
      " [ 92. 192.]]\n",
      "[[290.]\n",
      " [291.]\n",
      " [292.]]\n",
      "[[ 93. 193.]\n",
      " [ 94. 194.]\n",
      " [ 95. 195.]]\n",
      "[[293.]\n",
      " [294.]\n",
      " [295.]]\n",
      "[[ 96. 196.]\n",
      " [ 97. 197.]\n",
      " [ 98. 198.]]\n",
      "[[296.]\n",
      " [297.]\n",
      " [298.]]\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE = 3\n",
    "TRAINSIZE = 100\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCHSIZE).repeat()\n",
    "\n",
    "train_data_x1 = np.arange(0, 100).reshape((100, 1))\n",
    "train_data_x2 = np.arange(100, 200).reshape((100, 1))\n",
    "train_data_x = np.hstack([train_data_x1, train_data_x2])\n",
    "train_data_y = np.arange(200, 300).reshape((100, 1))\n",
    "\n",
    "iter = train_dataset.make_initializable_iterator() # create the iterator\n",
    "next_elements = iter.get_next()\n",
    "\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data_x, y: train_data_y, batch_size: 16})\n",
    "    for i in range(TRAINSIZE // BATCHSIZE):\n",
    "        features, labels = sess.run(next_elements)\n",
    "        print(features)\n",
    "        print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Me\\Anaconda3\\envs\\tf_gpu_p3.6\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:358: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-1c86b2b12a88>:22: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Me\\Anaconda3\\envs\\tf_gpu_p3.6\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1c86b2b12a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtot_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtot_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_batches' is not defined"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
