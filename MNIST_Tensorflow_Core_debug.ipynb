{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, depth):\n",
    "    depth = tf.constant(depth, tf.int32, name=\"depth\")\n",
    "    one_hot_matrix = tf.one_hot(labels, depth, axis=-1)\n",
    "        \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 60000\n",
      "number of test examples = 10000\n",
      "X_train shape: (60000, 784)\n",
      "Y_train shape: (60000, 10)\n",
      "X_test shape: (10000, 784)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New way\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = mnist.load_data()\n",
    "\n",
    "#Flatten the training and test images\n",
    "x_train_flat = x_train_orig.reshape(x_train_orig.shape[0], -1)\n",
    "x_test_flat = x_test_orig.reshape(x_test_orig.shape[0], -1)\n",
    "\n",
    "#Normalize image vectors\n",
    "x_train = (x_train_flat / 255.0).astype('float32')\n",
    "x_test = (x_test_flat / 255.0).astype('float32')\n",
    "\n",
    "#Convert training and test labels to on hot matrices\n",
    "num_classes = 10\n",
    "y_train = one_hot_matrix(y_train_orig, num_classes)\n",
    "y_test = one_hot_matrix(y_test_orig, num_classes)\n",
    "\n",
    "n_x = x_train.shape[1]\n",
    "n_y = x_test.shape[1]\n",
    "\n",
    "print (\"number of training examples = \" + str(x_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(x_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(x_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_test shape: \" + str(x_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))\n",
    "\n",
    "#print(x_train[0:2, :])\n",
    "#print(y_train[0:2, :])\n",
    "\n",
    "#print(x_train_flat.shape)\n",
    "\n",
    "x_train.dtype\n",
    "#x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def initialize_parameters(n_x):\n",
    "    W1 = tf.get_variable(\"W1\", [n_x, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [1, 100], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [100, 100], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [1, 100], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [100, 10], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [1, 10], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(X, W1) + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.matmul(A1, W2) + b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.matmul(A2, W3) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New way\n",
    "def compute_cost(Z, Y):   \n",
    "    logits = Z\n",
    "    labels = Y\n",
    "    \n",
    "    #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)\n",
    "    #cost = tf.reduce_mean(cross_entropy)            \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels)\n",
    "    cost = tf.reduce_mean(cross_entropy)    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.4005938, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "(m, n_x) = x_train.shape\n",
    "n_y = y_train.shape[1]\n",
    "\n",
    "#X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "parameters = initialize_parameters(n_x)\n",
    "\n",
    "Z = forward_propagation(x_train, parameters)\n",
    "\n",
    "cost = compute_cost(Z, y_train)\n",
    "print(cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
